{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/Users/emilieelisabethmilannielsen/anaconda3/envs/appmachine/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#How to hashtag alt på en gang:  alt + shift marker alt men kun ude i siden - slip! og så lav hashtag\n",
    "# marker et ord - command + D så kan man rette i alle ord samtidig\n",
    "\n",
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (121495, 166)\n",
      "Shape of test data set: (160651, 164)\n"
     ]
    }
   ],
   "source": [
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('data/train')\n",
    "test  = load_data('data/test')\n",
    "\n",
    "train = train.loc[train['Truth'] == 1]\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "#print(train['Truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n",
    "\n",
    "X = train[all_variables]\n",
    "y = train['p_truth_E']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#X_train_cut = X_train[:5000]\n",
    "#X_test_cut = X_test[:5000]\n",
    "#y_train_cut = y_train[:5000]\n",
    "#y_test_cut = y_test[:5000]\n",
    "\n",
    "X_train_cut = X_train\n",
    "X_test_cut = X_test\n",
    "y_train_cut = y_train\n",
    "y_test_cut = y_test\n",
    "\n",
    "# Reset the index of y_test_cut\n",
    "#y_test_cut = y_test_cut.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182534473102821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15,   0,   0,   0,   0,   0,  21,  58,  78,   6,  20,   5,  59,\n",
       "        10,  90,  73,  50,  89,  19,   8,   5,  12, 270,   0,   0,  53,\n",
       "         1,  20,  10,   5,   0,   2,   0,  12,   2,   0,   4,   1,   8,\n",
       "        13,  14,  13,  12,   2,  31,  37,  42,  32,   1,   9,  16,   8,\n",
       "       111, 142,   6,   6,   1,   5,   5,   0,   2,  19,  12,   0,   3,\n",
       "        12,   0,   0,   0,  16,   0,   9,  30,   0,  15,   1,   0,   0,\n",
       "        17,   7,   0,  10,  37,   1, 170,   9,   4,  42,  70,  15,  22,\n",
       "         2,  43,  17,   6,   3,   7,   2,  36,  57,  12,   0,  20,   2,\n",
       "         3,  19,  17,   0,  17,   7,   9,   2,   0,   0,   0,  61,   0,\n",
       "         4,  73,   0,   0,  13,   0,   0,   6,  24,  11,  20,   0,  28,\n",
       "        32,   0,  15,   7,  31,  34,   0,  25,  12,  11,   2,  46,  18,\n",
       "         7,  32,  67,   0,  22,   8,   9,   2,  18,   3,   8,   3,   3,\n",
       "         4,  19,   3,   3], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.booster_.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance of each attribute\n",
    "#fea_imp_ = pd.DataFrame({'cols':X.columns, 'fea_imp':model.feature_importances_})\n",
    "#chosen_variables = list(fea_imp_.loc[fea_imp_.fea_imp > 0].sort_values(by=['fea_imp'], ascending = False).iloc[:20]['cols'])\n",
    "chosen_variables = ['p_eCluster', 'p_deltaEta2', 'p_deltaPhiRescaled2', 'p_deltaEta1', 'p_d0', 'p_EptRatio', 'p_pt_track', 'p_sigmad0', 'p_nCells_Lr2_HiG', 'p_deltaPhi2', 'p_DeltaE', 'p_nCells_Lr1_HiG', 'p_qOverP', 'p_nTracks', 'p_ecore', 'p_rawECluster', 'p_d0Sig', 'p_e2tsts1', 'p_deltaPhiRescaled1', 'p_Rphi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p_eCluster', 'p_deltaEta2', 'p_deltaPhiRescaled2', 'p_deltaEta1', 'p_d0', 'p_EptRatio', 'p_pt_track', 'p_sigmad0', 'p_nCells_Lr2_HiG', 'p_deltaPhi2', 'p_DeltaE', 'p_nCells_Lr1_HiG', 'p_qOverP', 'p_nTracks', 'p_ecore', 'p_rawECluster', 'p_d0Sig', 'p_e2tsts1', 'p_deltaPhiRescaled1', 'p_Rphi']\n"
     ]
    }
   ],
   "source": [
    "X = train[chosen_variables]\n",
    "y = train['p_truth_E']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_cut = X_train\n",
    "X_test_cut = X_test\n",
    "y_train_cut = y_train\n",
    "y_test_cut = y_test\n",
    "print(chosen_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9186675122764327"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def DecisionTree_CrossValidation(max_depth, num_leaves, learning_rate, n_estimators, min_split_gain, data, targets):\n",
    "    \"\"\"Decision Tree cross validation.\n",
    "       Fits a Decision Tree with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations of max_depth, min_samples_leaf \n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = lgb.LGBMRegressor(max_depth=max_depth, random_state=42, num_leaves=num_leaves, learning_rate=learning_rate, n_estimators=n_estimators, min_split_gain=min_split_gain)\n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, scoring='neg_mean_absolute_error', cv=5)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_DecisionTree(data, targets, pars, n_iter=50):\n",
    "    \"\"\"Apply Bayesian Optimization to Decision Tree parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(max_depth, num_leaves, learning_rate, n_estimators, min_split_gain):\n",
    "        \"\"\"Wrapper of Decision Tree cross validation. \n",
    "           Notice how we ensure max_depth, min_samples_leaf \n",
    "           are casted to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return DecisionTree_CrossValidation(max_depth=int(max_depth), \n",
    "                                            num_leaves=int(num_leaves),\n",
    "                                            learning_rate=learning_rate, \n",
    "                                            n_estimators = int(n_estimators), \n",
    "                                            min_split_gain=min_split_gain,\n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=2)\n",
    "    optimizer.maximize(init_points=8, n_iter=n_iter)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_bayesian_opt_lightgbm(x, y, num_leaves_range, max_depth_range, learning_rate_range, n_estimators_range, min_split_gain_range, iters=10):\n",
    "\n",
    "    parameters_BayesianOptimization = {\"num_leaves\": num_leaves_range, \n",
    "                                    \"max_depth\": max_depth_range, \n",
    "                                    \"learning_rate\": learning_rate_range, \n",
    "                                    \"n_estimators\": n_estimators_range,\n",
    "                                    \"min_split_gain\": min_split_gain_range\n",
    "                                    }\n",
    "\n",
    "    BayesianOptimization = optimize_DecisionTree(x, \n",
    "                                                y, \n",
    "                                                parameters_BayesianOptimization, \n",
    "                                                n_iter=iters)\n",
    "    print(BayesianOptimization.max)\n",
    "    dict = BayesianOptimization.max['params']\n",
    "    dict['max_depth'] = int(round(dict['max_depth']))\n",
    "    dict['n_estimators'] = int(round(dict['n_estimators']))\n",
    "    dict['num_leaves'] = int(round(dict['num_leaves']))\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | min_sp... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-6.283e+0\u001b[0m | \u001b[0m0.1935   \u001b[0m | \u001b[0m192.6    \u001b[0m | \u001b[0m43.92    \u001b[0m | \u001b[0m139.8    \u001b[0m | \u001b[0m28.26    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-6.235e+0\u001b[0m | \u001b[95m0.08644  \u001b[0m | \u001b[95m58.71    \u001b[0m | \u001b[95m51.97    \u001b[0m | \u001b[95m140.2    \u001b[0m | \u001b[95m75.19    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-1.03e+04\u001b[0m | \u001b[0m0.02009  \u001b[0m | \u001b[0m195.5    \u001b[0m | \u001b[0m49.95    \u001b[0m | \u001b[0m81.85    \u001b[0m | \u001b[0m30.46    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m-6.188e+0\u001b[0m | \u001b[95m0.09987  \u001b[0m | \u001b[95m95.64    \u001b[0m | \u001b[95m31.49    \u001b[0m | \u001b[95m114.8    \u001b[0m | \u001b[95m39.75    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-6.548e+0\u001b[0m | \u001b[0m0.3098   \u001b[0m | \u001b[0m70.92    \u001b[0m | \u001b[0m17.53    \u001b[0m | \u001b[0m105.0    \u001b[0m | \u001b[0m53.77    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-6.521e+0\u001b[0m | \u001b[0m0.3947   \u001b[0m | \u001b[0m79.95    \u001b[0m | \u001b[0m30.85    \u001b[0m | \u001b[0m138.9    \u001b[0m | \u001b[0m18.95    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-6.95e+03\u001b[0m | \u001b[0m0.3077   \u001b[0m | \u001b[0m75.58    \u001b[0m | \u001b[0m3.903    \u001b[0m | \u001b[0m192.3    \u001b[0m | \u001b[0m97.08    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-6.903e+0\u001b[0m | \u001b[0m0.4061   \u001b[0m | \u001b[0m95.69    \u001b[0m | \u001b[0m5.86     \u001b[0m | \u001b[0m152.6    \u001b[0m | \u001b[0m52.41    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-6.5e+03 \u001b[0m | \u001b[0m0.3116   \u001b[0m | \u001b[0m184.4    \u001b[0m | \u001b[0m40.07    \u001b[0m | \u001b[0m183.0    \u001b[0m | \u001b[0m26.56    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-6.679e+0\u001b[0m | \u001b[0m0.3812   \u001b[0m | \u001b[0m183.3    \u001b[0m | \u001b[0m42.61    \u001b[0m | \u001b[0m181.5    \u001b[0m | \u001b[0m28.79    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-6.589e+0\u001b[0m | \u001b[0m0.4199   \u001b[0m | \u001b[0m179.1    \u001b[0m | \u001b[0m1.613    \u001b[0m | \u001b[0m165.9    \u001b[0m | \u001b[0m18.29    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-7.242e+0\u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m95.22    \u001b[0m | \u001b[0m60.0     \u001b[0m | \u001b[0m113.1    \u001b[0m | \u001b[0m76.24    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-1.329e+0\u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m59.46    \u001b[0m | \u001b[0m122.3    \u001b[0m | \u001b[0m37.23    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-6.821e+0\u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m89.88    \u001b[0m | \u001b[0m4.212    \u001b[0m | \u001b[0m120.2    \u001b[0m | \u001b[0m28.45    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-6.435e+0\u001b[0m | \u001b[0m0.1786   \u001b[0m | \u001b[0m74.52    \u001b[0m | \u001b[0m27.57    \u001b[0m | \u001b[0m141.6    \u001b[0m | \u001b[0m85.17    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-6.372e+0\u001b[0m | \u001b[0m0.1312   \u001b[0m | \u001b[0m60.04    \u001b[0m | \u001b[0m53.91    \u001b[0m | \u001b[0m159.2    \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-6.305e+0\u001b[0m | \u001b[0m0.2297   \u001b[0m | \u001b[0m119.6    \u001b[0m | \u001b[0m33.18    \u001b[0m | \u001b[0m139.1    \u001b[0m | \u001b[0m21.69    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-7.318e+0\u001b[0m | \u001b[0m0.4723   \u001b[0m | \u001b[0m87.33    \u001b[0m | \u001b[0m58.08    \u001b[0m | \u001b[0m169.8    \u001b[0m | \u001b[0m74.77    \u001b[0m |\n",
      "=====================================================================================\n",
      "{'target': -6187.598582683643, 'params': {'learning_rate': 0.09986820982818256, 'max_depth': 95.63633644393066, 'min_split_gain': 31.485385897934272, 'n_estimators': 114.79175279631737, 'num_leaves': 39.75447691683357}}\n"
     ]
    }
   ],
   "source": [
    "param_dict = run_bayesian_opt_lightgbm(X_train_cut, y_train_cut, (15,100), (50,200), (0.01,0.5), ( 50,200),(0,60), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.09578523237744975, 'max_depth': 152.8168792368093, 'min_split_gain': 31.485385897934272, 'n_estimators': 189.81965689758283, 'num_leaves': 19.600060991288387}\n"
     ]
    }
   ],
   "source": [
    "params = BayesianOptimization.max['params']\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = lgb.LGBMRegressor(max_depth=int(152.8168792368093), random_state=42, min_split_gain=31.485385897934272, num_leaves=int(19.600060991288387), learning_rate=0.09578523237744975, n_estimators=int(189.81965689758283))\n",
    "model = lgb.LGBMRegressor(**param_dict)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9200830005614982"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06914363942352776\n"
     ]
    }
   ],
   "source": [
    "def MAE(E_pred,E_true):\n",
    "    return np.mean(np.abs(E_pred-E_true)/E_true)\n",
    "\n",
    "MAE = MAE(y_pred,y_test)\n",
    "\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appmachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
